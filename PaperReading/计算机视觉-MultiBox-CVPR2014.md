---
title: MultiBox-CVPR2014
sitemap: true
categories: 计算机视觉
date: 2018-10-14 15:41:07
tags:
- 计算机视觉
- 目标检测
---

**文章:** Scalable Object Detection using Deep Neural Networks
**作者:** Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov

# 核心亮点

**(1) 回归问题:**
将物体检测问题定义为输出多个bounding box的回归问题. 同时每个bounding box会输出关于是否包含目标物体的置信度, 使得模型更加紧凑和高效

**(2) 损失函数:**
将训练bounding box检测器作为整个网络训练过程的一部分, 也就是说在损失函数中包含了关于bounding box的损失项. 通过联合训练, 不仅利用了神经网络强大的特征表示能力, 而且将检测器的训练集成到了网络中

**(3) 无类别监督训练**
作者将本文的目标边框检测器在无监督的样本下训练, 由于本方法主要完成的功能就是画框, 并不会输出框中包含的物体类别, 因此训练的时候无需知道样本的类别信息. 这也使得该方法的计算复杂度与类别信息几乎无关, 可以轻易的推广到未知的类别当中. (当然也可以进行相关类别的训练, 对每个类别都训练一个检测器, 模型的总参数会随着类别数线性增加)

**关键技术**

作者将bounding box的检测过程集成到了神经网络中, 使其转变成了一个回归问题, 通过BP算法优化下面的损失函数即可获得预测的框, 相比于SS算法, 计算复杂度更低.

$x_{ij}=1$ 当且仅当第 $i$ 个预测框与第 $j$ 个真实框匹配. $l_i$ 和 $g_j$ 分别是预测框和真实框的归一化后的坐标, $c_i$ 代表置信度:

$$F_{match}(x,l) = \frac{1}{2} \sum_{i,j} x_{ij} \|l_i - g_j\|_2^2$$

$$F_{conf}(x,c) = -\sum{i,j} x_{i,j} log(c_i) - \sum_i (1 - \sum_j x_{ij}) log(1-c_i)$$

$$F(x,l,c) = \alpha F_{match}(x,l) + F_{conf}(x,c)$$

$$x^* = \arg \min_x F(x,l,c)$$
$$ \text{subject to } x_{ij} \in \{0, 1\}, \sum_i x_{ij}=1$$

利用BP算法分别对 $l_i$ 和 $c_i$ 求导, 以便更新相关参数使其损失函数值更低.

$$\frac{\partial F}{\partial l_i} = \sum_j (l_i - g_j) x^*_{ij}$$

$$\frac{\partial F}{\partial c_i} = \frac{\sum_j x^*_{ij} c_i}{c_i(1-c_i)}$$

# 论文细节

## 背景介绍

在(2014年)之前的工作中, 对于目标检测任务都是对整个图片进行检测, 无法检测出同一张图片中的多个目标物. 于是, 本文就提出了一种目标检测模型, 可以在一张图片中预测多个bounding boxes, 并且每个box都对应了包含某个类别物体的置信度.

作者使用了一个单一的DNN网络, 来生成候选区域框, 并且每个区域框都会带有一个置信度, 代表这框内包含物体的可能性大小.

**Model:** 模型最后一层的神经元的输出值代表着每个框的坐标和对应的置信度.

**Bounding Box:** 将左上角和右下角的坐标分别作为四个神经元的输出值. 这些坐标都是经过归一化的.

**Confidence:** 每个Box对应的置信度会单独作为一个神经元节点输出.

在预测阶段, 可以利用该模型输出 $K$ 个bounding box预测结果, 同时可以利用NMS算法得到置信度更高的Box集合, 然后将这些集合送到分类器中进行分类.

**训练目标:** 假设对于一个训练样本, 具有 $M$ 个已经标注好的GT bounding box. 然后, 检测器会生成 $K$ 个预测的bounding box, $K$ 的值一般远远大于 $M$. 因此, 我们仅仅需要优化 $K$ 中与 $M$ 个GT匹配度最高的一个子集合. 优化的时候, 我们尽可能的提高这些子集合内部的预测框的置信度, 同时降低其他那些不在子集合里面的框的置信度. 对此, 形式化描述为下面的函数:

$$F_{match}(x,l) = \frac{1}{2} \sum_{i,j} x_{ij} \|l_i - g_j\|_2^2$$

上式中, $x_{ij}=1$ 当且仅当第 $i$ 个预测框与第 $j$ 个真实框匹配. $l_i$ 和 $g_j$ 分别是预测框和真实框的归一化后的坐标.

此外, 我们还希望对预测框的置信度进行优化, 将匹配框的置信度最大化, 这个过程转换成最小化下面的式子:

$$F_{conf}(x,c) = -\sum{i,j} x_{i,j} log(c_i) - \sum_i (1 - \sum_j x_{ij}) log(1-c_i)$$

从上式可以看到, $\sum_j x_{ij} = 1$ 当且仅当预测框 $i$ 可以匹配到某个真实框. 在这种情况下, $c_i$ 将王越来越大的方向优化. 上面这个式子正式交叉熵.

结合上面的两个公式, 最终的损失函数如下所示, 其中 $\alpha$ 用于调节两部分的权重:

$$F(x,l,c) = \alpha F_{match}(x,l) + F_{conf}(x,c)$$

**优化:** 对于每一个训练样本, 都希望按照如下最优化问题求得 $x^*$ (也就是最优化预测框与真实框的匹配方案) :

$$x^* = \arg \min_x F(x,l,c)$$
$$ \text{subject to } x_{ij} \in \{0, 1\}, \sum_i x_{ij}=1$$

由于标记物体的数量非常少, 所以上面公式的计算复杂度并不高. 对于上面的公式, 可以利用BP算法分别对 $l_i$ 和 $c_i$ 求导, 以便更新相关参数使其损失函数值更低.

**Training Details:**

使用了三个小改动, 进一步提升了精度的速度
